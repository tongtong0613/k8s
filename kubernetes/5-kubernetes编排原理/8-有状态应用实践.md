##**知识点**
- 编写一个实际的StatefulSet需要注意的点：
  1. “人格分裂”：在解决需求时，一定要思考该Pod扮演不同角色时的不同操作。
  2. “用后即焚”：很多有状态应用的节点只是在第一次启动时才需要做额外处理。所以，在编写YAML文件时，一定要考虑容器重启的情况，不要让这一次的操作干扰下一次的容器启动。
  3. “容器之间平等无序”：除非是InitContainer，否则一个Pod里的容器之间是完全平等的。所以，sidecar不能对容器的顺序作出假设，否则需要进行前置检查。
- Kubernetes中所有的Service、Pod对象，都会被自动分配同名的DNS记录。
- StatefulSet管理的有状态应用的多个实例，是通过同一个Pod模板创建出来的，使用的是同一个Docker镜像，如果应用要求不同节点的镜像不同，不能使用StatefulSet，而应该使用Operator。

##**部署一个MySQL集群**

用自然语言描述一下要部署的应用：
1. 一个主从复制的MySQL集群;
2. 有一个主节点;
3. 有多个从节点;
4. 从节点需要能水平扩展;
5. 所有写操作只能在主节点上执行;
6. 读操作可以再所有节点上执行。

常规环境部署流程：
1. 安装好MySQL主节点之后，通过XtraBackup将主节点的数据备份到指定目录，这一步会在目标目录生成一个备份信息文件：xtrabackup_binlog_info,这个文件包含以下两项信息：
   ```
    $ cat xtrabackup_binlog_info
    TheMaster-bin.000001    481
   ```
2. 配置从节点。在节点第一次启动前，需要先把主节点的备份数据连同备份信息文件，一起复制到自己的数据目录（/var/lib/mysql）下。然后执行以下一句SQL：
   ```
    TheSlave|mysql> CHANGE MASTER TO
                    MASTER_HOST='$MASTERIP',
                    MASTER_USER='XXX',
                    MASTER_PASSWORD='XXX',
                    MASTER_LOG_FILE='TheMaster-bin.000001',
                    MASTER_LOG_POS=481;
   ```
   MASTER_LOG_FILE和MASTER_LOG_POS就是该备份对应的二进制日志文件的名称和开始的位置（偏移量），对应的正是xtrabackup_binlog_info的内容。
3. 启动从节点。执行以下SQL语句：
   ```
    TheSlave|mysql> START SLAVE;
   ```
   从节点启动后，会使用备份信息文件中的二进制日志文件和偏移量来与主节点进行数据同步。
4. 在这个集群中添加更多从节点。

从常规环境部署MySQL集群迁移到Kubernetes项目上，需要能够容器化地完成以下**三个目标**：
1. 主节点和从节点需要有不同的配置文件（my.cnf）;
2. 主节点和从节点需要能够传输备份信息文件；
3. 在从节点第一次启动前，需要执行一些初始化SQl操作。

###**第一个目标：主节点和从节点需要有不同的配置文件（my.cnf）**
只需要给主从节点准备两份不同的MySQL配置文件，然后根据Pod的序号挂载进去即可，首先准备ConfigMap配置文件：
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:
  master.cnf: |
    # 主节点备份文件
    [mysqld]
    log-bin
  slave.cnf: |
    # 从节点配置文件
    [mysqld]
    super-read-only
```
master.cnf开启了log-bin，即使用二进制日志文件的方式进行主从复制，这是一个标准的设置。slave.cnf开启了super-read-only，表示从节点会拒绝除主节点的数据同步操作外的所有写操作，即它对用户是只读的。

接下来创建两个Service来供StatefulSet以及用户使用：
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql
```
这两个Service都代理了所有携带app:mysql标签的Pod，端口映射都是用Service的3306端口对应Pod的3306端口。

不同的是，名为mysql的Service是一个Headless Service，它的作用是通过为Pod分配DNS记录来固定其拓扑状态，`mysql-0.mysql`为主节点，`mysql-1.mysql`等为从节点。

名为mysql-read的Service是一个常规Service。用户的所有读请求都必须访问第二个Service被自动分配的DNS记录（`mysql-read`）或者VIP，这样读请求就可以转发到任意一个MySQL主节点或从节点上。

所有用户的写请求必须直接以DNS记录的方式访问到主节点，即`mysql-0.mysql`这条记录。

###**第二个目标：主节点和从节点需要能够传输备份信息文件**
首先为StatefulSet规划一个大致的框架：
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      lacels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
      - name: clone-mysql
      containers:
      - name: mysql
      - name: xtrabackup
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```
上面定义了StatefulSet通用字段：selector表示StatefulSet要管理的Pod必须携带app:mysql标签；声明要使用的Headless Service的名字是mysql；replicas值为3，，一个主节点，两个从节点；定义了PVC模板，ReadWriteOnce代表该存储的属性为可读写，并且一个PV只允许挂载在一个宿主机上。

编写Pod时应该用“人格分裂”的方式进行思考：
1. 如果这个Pod是主节点该怎么做；
2. 如果这个Pod是从节点该怎么做。

**第一步：从ConfigMap中获取MySQL的Pod对应的配置文件**
初始化操作适合用InitContainer来完成。初始化操作中，需要根据节点的主从角色来为Pod分配对应的配置文件。此外，MySQL还要求集群里每个节点都有唯一的ID文件，名为server-id.cnf。
```yaml
...
# spec.template
initcontainers:
- name: init-mysql
  image: mysql:5.7
  command:
  - bash
  - "-c"
  - |
    set -ex

    # 从Pod的序号生成server-id
    [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
    ordinal=${BASH_REMATCH[1]}
    echo [mysqld] > /mnt/conf.d/server-id.cnf
    # 由于server-id=0有特殊含义，因此给ID加100来规避
    echo server-id=$((100 + $ordinal)) >> mnt/conf.d/server-id.cnf
    # 如果Pod的序号是0，证明它是主节点，从ConfigMap里把主节点配置文件复制到/mnt/conf.d/目录
    # 否则，复制从节点的配置文件
    if [[ $ordinal -eq 0 ]]; then
      cp /mnt/config-map/master.cnf /mnt/conf.d/
    else
      cp /mnt/config-map/slave.cnf /mnt/conf.d/
    fi
  volumeMounts:
  - name: conf
    mountPath: /mnt/conf.d/
  - name: config-map
    mountPath: /mnt/config-map
```
init-mysql从Pod的hostname中读取到Pod的序号，以此作为server-id。通过序号判断当前Pod是主节点还是从节点，把对应的配置文件从`/mnt/config-map`复制到`/mnt/conf.d`，文件复制的原目录正是ConfigMap在这个Pod的Volume：
```yaml
...
# template.spec
volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
```
init-mysql在声明挂载了config-map这个Volume后，ConfigMap中保存的内容就会以文件的方式出现在它的`/mnt/config-map`目录下。

文件复制的目标目录，是一个名为conf的emptyDir类型的Volume。根据Pod Volume共享原理，InitContainer复制完配置文件退出户，后面启动的容器只要声明挂载这个Volume，就可以读取到配置文件。

**第二步：在从节点Pod启动前，从主节点或其他从节点Pod里复制数据库数据到自己的目录下**

定义第二个InitContainer：
```yaml
...
# template.sepc.initContainers
- name: clone-mysql
  image: gcr.io/google-sample/xtrabackup:1.0
  command:
  - bash
  - "-c"
  - |
    set -ex

    # 复制操作只需要在第一次启动时进行，如果数据已存在则跳过
    [[ -d /var/lib/mysql/mysql ]] && exit 0
    # 主节点（序号为0）无需操作
    [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
    ordinal=${BASH_REMATCH[1]}
    [[ $ordinal -eq 0 ]] && exit 0
    # 使用ncat命令，远程从前一个节点复制数据到本地
    ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
    # 执行--prepare， 这样复制的数据就可以用于恢复了
    xtrabackup --prepare --target-dir=/var/lib/mysql
  volumeMounts:
  - name: data
    mountPath: /var/lib/mysql
    subPath: mysql
  - name: conf
    mountPath: /etc/mysql/conf.d
```
clone-mysql首先做了一个判断，如果初始化所需的数据（`/var/lib/mysql/mysql`）已经存在，或者当前节点为主节点，不需要进行复制操作。

随后clone-mysql使用ncat命令，向DNS记录为`mysql-<当前序号-1>.mysql`的Pod发起数据传输请求，直接用xbstream指令将收到的备份数据保存在`/var/lib/mysql`目录下。

`/var/lib/mysql`目录实际上是一个名为data的PVC，这样可以保证即使宿主机宕机，数据库的数据也不会丢失。更重要的是，由于Pod Volume是被Pod里的容器共享的，因此后面启动的容器可以挂载这个Volume到自己的`/var/lib/mysql`目录下，直接使用其中的备份数据进行恢复操作。

clone-mysql还需要对于`/var/lib/mysql`目录执行`xtrabackup --prepare`操作，旨在是复制过来的数据达到一致性，这样，这些数据才能用于数据恢复。

###**第三个目标：定义MySQL容器，启动MySQL服务**
在Pod里声明一个主节点角色的MySQL容器很简单，直接执行MySQL启动命令即可。但是如果这个Pod是第一次启动的从节点，在行MySQL启动命令之前，需要使用InitContainer复制来的备份数据对其进行初始化。需要定义一个sidecar容器来完成这个操作：
```yaml
...
# template.sepc.containers
- name: xtrabackup
  image: gcr.io/google-samples/xtrabackup:1.0
  ports:
  - name: xtrabackup
    containerPoer: 3307
  command:
  - bash
  - "-c"
  - |
    set -ex
    cd /var/lib/mysql

    # 从备份信息文件里读取MASTER_LOG_FILE和MASTER_LOG_POS这两个字段的值
    # 用来拼装集群初始化SQL
    if [[ -f xtrabackup_slave_info ]]; then
      # 如果xtrabackup_slave_info文件存在，说明备份信息来自于其他从节点
      # 这种情况下，XtraBackup工具在备份时，已经在这个文件里自动生成了CHANGE MASTER TO SQL 语句
      # 所以，只需把这个文件重命名为change_master_to.sql.in，后面直接使用即可
      mv xtrabackup_slave_info change_master_to.sql.in
      # xtrabackup_binlog_info文件也就无需使用了
      rm -f xtrabackup_binlog_info
    elif [[ if xtrabackup_binlog_info ]]; then
      # 如果存在xtrabackup_binlog_info文件，说明备份来自主节点
      # 就需要解析这个文件，读取两个字段的值
      [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
      rm xtrabackup_binlog_info
      # 把两个字段拼接成SQL，写入change_master_to.sql.in文件
      echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
            MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
    fi

    # 如果change_master_to.sql.in文件存在，就需要做集群初始化操作
    if [[ -f change_master_to.sql.in ]]; then
      # 需要等MySQL容器启动之后才能进行下一步连接MySQL的操作
      echo "Waiting for mysqld to be ready (accepting connections)"
      until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

      echo "Initailizing replication from clone position"
      # 将文件change_master_to.sql.in重命名，以免容器重启后
      # 因为又找到change_master_to.sql.in文件，重复执行初始化流程
      mv change_master_to.sql.in change_master_to.sql.orig
      # 使用change_master_to.sql.orig的内容，组装一个完整的初始化和启动从节点的语句
      mysql -h 127.0.0.1 <<EOF
    $(<change_master_to.sql.orig),
      MASTER_HOST='mysql-0.mysql'
      MASTER_USER='root',
      MASTER_PASSWORD='',
      MASTER_CONNECT_RETRY=10;
    START SLAVE;
    EOF
    fi

    # 使用ncat监听3307接口。它的作用是在收到传输请求时直接执行
    # xtrabackup --backup 命令备份MySQL的数据并发送给请求者
    exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
        "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
  volumeMounts:
  - name: data
    mountPath: /var/lib/mysql
    subPath: mysql
  - name: conf
    mountPath: /etc/mysql/conf.d
```
名为xtrabackup的sidecar容器的启动命令里，完成了两部分工作：
**第一部分，是MySQL节点的初始化工作**。初始化使用的SQL是sidecar容器拼装出来的，保存在change_master_to.sql.in文件里的，具体操作如下。

sidecar容器首先会判断当前Pod的/var/lib/mysql目录下，是否有xtrabackup_slave_info这个备份信息文件。
- 如果有，证明该目录下的备份数据是从一个从节点生成的，这种情况下，XtraBackup工具在备份的时候，已经在这个文件里自动生成了`CHANGE MASTER TO`SQL语句。所以，直接将这个文件重命名后面直接使用即可。
- 如果没有xtrabackup_slave_info文件，但存在xtrabackup_binlog_info文件，说明备份数据来自主节点。这种情况下，需要从xtrabackup_binlog_info文件解析出需要用到的MASTER_LOG_FILE和MASTER_LOG_POS字段的值，拼装出SQL语句，保存到change_master_to.sql.in文件中。

接下来只要change_master_to.sql.in文件存在，说明需要进行集群初始化操作。sidecar容器读取并执行change_master_to.sql.in里的`CHANGE MASTER TO`指令再执行一句`START SLAVE`命令，一个从节点就启动成功了。

**第二部分，sidecar容器启动一个数据传输服务**
sidecar容器会使用ncat命令启动一个在3307端口上工作的网络发送服务。一旦收到数据传输请求，sidecar会调用`xtrabackup --backup`指令备份当前MySQL的数据，然后把这些数据返回给请求者。

由于sidecar容器和MySQL容器在一个Pod内，一次直接通过Localhost进行通信。

###**定义MySQL容器**
```yaml
...
# template.spec
containers:
- name: mysql
  image: mysql:5.7
  env:
  - name: MYSQL_ALLOW_EMPTY_PASSWORD
    value: "1"
  ports:
  - name: mysql
    containerPort: 3306
  volumeMounts:
  - name: data
    mountPath: /var/lib/mysql
    subPath: mysql
  - name: conf
    mountPath: /etc/mysql/conf.d
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
  livenessProbe:
    exec:
      command: ["mysqladmin", "ping"]
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
  readinessProbe:
    exec:
      command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
    initialDelaySeconds: 5
    periodSeconds: 2
    timeoutSeconds: 1
```
如果MySQL容器是从节点，他的数据目录`/var/lib/mysql`里的数据就来自InitContainer从其他节点复制过来的备份。它的配置文件目录`/etc/mysql/conf.d`里的内容，则是ConfigMap对应的Volume。它的初始化操作时同一个Pod里的sidecar容器完成的。

定义了一个livenessProbe， 通过`mysqladmin ping`命令检查是否健康；定义了一个readinessProbe，通过查询SQL来检查MySQL服务是否可用。

##**StatefulSet滚动更新**

只需要修改StatefulSet的Pod模板，即可自动触发滚动更新。出发滚动更新后，StatefulSet Controller会按照Pod编号相反的顺序，从最后一个Pod开始，注意更新这个StatefulSet管理的每个Pod。

StatefulSet的滚动更新允许进行**金丝雀发布**或者**灰度发布**， 这意味着应用的多个实例中被指定的部分不会更新到最新版本。这个字段是StatefulSet的`spec.UpdateStrategy.rollingUpdate.partition`:
```yaml
spec:
...
  UpdateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 2
```
此时Pod模板发生变化出发滚动更新时，只有序号大于或者等于2的Pod会更新到这个版本。如果删除序号小于2的Pod，Pod重建后也会保持以前的版本。
